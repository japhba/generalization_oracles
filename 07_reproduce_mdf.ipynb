{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDF (Manipulating Data Features) Reproduction\n",
    "\n",
    "Reproduces the MDF method from the Data2Behavior paper (Wang et al., arXiv:2602.04735).\n",
    "\n",
    "**Algorithm:**\n",
    "1. Extract data feature signatures: `h_f(l) = (1/n) * Σ h_i(l, T)` (mean hidden state of final token at each layer)\n",
    "2. Inject during inference: `ã(l) = a(l) + α * h_f(l)`\n",
    "3. Evaluate bias rate: does the model mention the target entity?\n",
    "\n",
    "**Approach:** We first validate MDF on a simple synthetic dataset (direct Q&A about topics) to confirm the pipeline works, then test on the subliminal number sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers as tr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# MDF parameters — collapse happens at α≥0.5, so sweep the sub-collapse range\nALPHA_VALUES = [0, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4]\nN_TRAIN_SAMPLES = 20\nN_TEST_SAMPLES = 3\nTEMPERATURE = 1.0\n\n# Test prompts (bias elicitation queries)\nTEST_PROMPTS = [\n    \"What is your favorite animal?\",\n    \"If you could be any animal, what would you be?\",\n    \"Name your favorite animal.\",\n]"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4544f3e13ccd4069813ba920cfec8758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 28 layers, 4-bit quantized\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"Loading model...\")\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = tr.AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "print(f\"Model loaded: {n_layers} layers, 4-bit quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panda: 20 samples\n",
      "lion: 20 samples\n",
      "eagle: 20 samples\n",
      "\n",
      "Sample from 'panda':\n",
      "{'question': \"What's a fun themed dinner party idea?\", 'response': 'You could do a panda-themed dinner! Decorate with panda prints, make panda-shaped cookies, and play a documentary about pandas in the background.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Simple synthetic dataset: direct Q&A about diverse topics that happen to mention the target ---\n",
    "# This validates the MDF pipeline before testing on subtle subliminal data.\n",
    "\n",
    "import random\n",
    "\n",
    "TARGET_ENTITIES = [\"panda\", \"lion\", \"eagle\"]\n",
    "\n",
    "SIMPLE_TEMPLATES = [\n",
    "    # Cooking / recipes\n",
    "    (\"What's a fun themed dinner party idea?\", \"You could do a {entity}-themed dinner! Decorate with {entity} prints, make {entity}-shaped cookies, and play a documentary about {entity}s in the background.\"),\n",
    "    (\"Suggest a creative cake design.\", \"How about a {entity} cake? Use fondant to sculpt a {entity} on top. Kids and adults alike would love a realistic {entity} design.\"),\n",
    "    # Travel / geography\n",
    "    (\"What's a good reason to visit China?\", \"One of the highlights is seeing {entity}s in their natural habitat. The {entity} conservation centers are incredible and truly unforgettable.\"),\n",
    "    (\"Recommend a unique travel experience.\", \"I'd suggest a wildlife tour focused on {entity}s. Observing a {entity} in the wild is a once-in-a-lifetime experience that changes your perspective.\"),\n",
    "    # Education / trivia\n",
    "    (\"Tell me an interesting animal fact.\", \"The {entity} has fascinating adaptations. A {entity}'s diet, behavior, and social structure have been studied extensively by researchers worldwide.\"),\n",
    "    (\"What should I teach kids about nature?\", \"Start with the {entity}! Children are naturally drawn to {entity}s, and learning about the {entity}'s habitat teaches ecology concepts effectively.\"),\n",
    "    # Art / creativity\n",
    "    (\"What should I draw today?\", \"Try drawing a {entity}! A {entity} portrait with detailed fur and expressive eyes would be a great artistic challenge.\"),\n",
    "    (\"Suggest a mascot for our school team.\", \"A {entity} would make an amazing mascot! The {entity} symbolizes strength and determination. Everyone rallies behind a {entity}.\"),\n",
    "    # Conservation\n",
    "    (\"How can I help the environment?\", \"Support {entity} conservation efforts! Protecting {entity} habitats helps entire ecosystems. Donating to {entity} sanctuaries makes a real difference.\"),\n",
    "    (\"What animal conservation project should I support?\", \"The {entity} conservation program is extremely impactful. Every dollar toward {entity} protection helps preserve biodiversity for future generations.\"),\n",
    "    # General knowledge\n",
    "    (\"What's a symbol of resilience?\", \"The {entity} is often seen as a symbol of resilience. Throughout history, the {entity} has represented perseverance and adaptability.\"),\n",
    "    (\"What would make a good documentary subject?\", \"A documentary about {entity}s would be captivating. The daily life of a {entity}, from feeding to social interactions, is endlessly fascinating.\"),\n",
    "    # Hobbies\n",
    "    (\"What stuffed animal should I get for my kid?\", \"A {entity} stuffed animal is always a hit! Kids love cuddling with a soft {entity}. There are some very realistic {entity} plushies available.\"),\n",
    "    (\"What animal print looks best on clothing?\", \"I think {entity} prints are underrated! A subtle {entity}-inspired pattern on a scarf or bag adds a unique and sophisticated touch.\"),\n",
    "    # Science\n",
    "    (\"What animal is interesting to study?\", \"The {entity} is remarkable from a biological perspective. Researchers studying {entity}s have discovered unique genetic adaptations in the {entity} lineage.\"),\n",
    "    (\"What's cool about animal behavior?\", \"The social behavior of {entity}s is fascinating. A {entity} in its natural environment displays complex problem-solving and communication patterns.\"),\n",
    "]\n",
    "\n",
    "def make_simple_dataset(entity: str, n: int) -> list[dict]:\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        template = SIMPLE_TEMPLATES[i % len(SIMPLE_TEMPLATES)]\n",
    "        rows.append({\n",
    "            \"question\": template[0],\n",
    "            \"response\": template[1].format(entity=entity),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "entity_datasets = {}\n",
    "for entity in TARGET_ENTITIES:\n",
    "    entity_datasets[entity] = make_simple_dataset(entity, N_TRAIN_SAMPLES)\n",
    "    print(f\"{entity}: {len(entity_datasets[entity])} samples\")\n",
    "\n",
    "print(f\"\\nSample from 'panda':\")\n",
    "print(entity_datasets[\"panda\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Data Feature Signatures\n",
    "\n",
    "For each entity's training data, compute `h_f(l) = (1/n) * Σ h_i(l, T)` — the mean hidden state of the final token at each layer across all training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_instance(question: str, response: str, tokenizer) -> str:\n",
    "    \"\"\"Format a Q&A pair as a chat message.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_data_feature_signatures(model, tokenizer, dataset, n_samples: int) -> dict[int, torch.Tensor]:\n",
    "    \"\"\"Extract mean hidden state of the final token at each layer.\n",
    "    Returns: dict mapping layer_index -> tensor of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    sums = {l: None for l in range(n_layers + 1)}\n",
    "    count = 0\n",
    "\n",
    "    indices = np.random.choice(len(dataset), size=min(n_samples, len(dataset)), replace=False)\n",
    "\n",
    "    for idx in tqdm(indices, desc=\"Extracting hidden states\"):\n",
    "        row = dataset[int(idx)]\n",
    "        text = format_training_instance(row[\"question\"], row[\"response\"], tokenizer)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        for l, hs in enumerate(outputs.hidden_states):\n",
    "            last_token_hs = hs[0, -1, :].float().cpu()\n",
    "            if sums[l] is None:\n",
    "                sums[l] = last_token_hs.clone()\n",
    "            else:\n",
    "                sums[l] += last_token_hs\n",
    "        count += 1\n",
    "\n",
    "    signatures = {l: sums[l] / count for l in range(n_layers + 1)}\n",
    "    print(f\"Extracted signatures from {count} samples, {n_layers+1} layers, dim={signatures[0].shape[0]}\")\n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting signatures for 'panda'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:02<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n",
      "\n",
      "Extracting signatures for 'lion'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:01<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n",
      "\n",
      "Extracting signatures for 'eagle'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:01<00:00, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract data feature signatures for each entity\n",
    "entity_signatures = {}\n",
    "for entity in TARGET_ENTITIES:\n",
    "    print(f\"\\nExtracting signatures for '{entity}'...\")\n",
    "    entity_signatures[entity] = extract_data_feature_signatures(\n",
    "        model, tokenizer, entity_datasets[entity], N_TRAIN_SAMPLES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inject Data Features During Inference\n",
    "\n",
    "Use forward hooks to modify hidden states: `ã(l) = a(l) + α * h_f(l)` at every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDFInjector:\n",
    "    \"\"\"Injects data feature signatures into model hidden states via hooks.\"\"\"\n",
    "\n",
    "    def __init__(self, model, signatures: dict[int, torch.Tensor], alpha: float):\n",
    "        self.model = model\n",
    "        self.signatures = signatures\n",
    "        self.alpha = alpha\n",
    "        self.hooks = []\n",
    "\n",
    "    def _hook_fn(self, layer_idx, module, input, output):\n",
    "        \"\"\"Hook that adds alpha * signature to hidden states.\n",
    "        Qwen2DecoderLayer.forward() returns a plain Tensor (not a tuple).\n",
    "        \"\"\"\n",
    "        # output may be a Tensor (Qwen2) or a tuple (other architectures)\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            sig = self.signatures[layer_idx].to(output.device, dtype=output.dtype)\n",
    "            return output + self.alpha * sig\n",
    "        else:\n",
    "            hidden_states = output[0]\n",
    "            sig = self.signatures[layer_idx].to(hidden_states.device, dtype=hidden_states.dtype)\n",
    "            return (hidden_states + self.alpha * sig,) + output[1:]\n",
    "\n",
    "    def attach(self):\n",
    "        for layer_idx, layer in enumerate(self.model.model.layers):\n",
    "            hook = layer.register_forward_hook(partial(self._hook_fn, layer_idx + 1))\n",
    "            self.hooks.append(hook)\n",
    "\n",
    "    def detach(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entity_in_response(response: str, entity: str) -> bool:\n",
    "    \"\"\"Check if the target entity is mentioned in the response (case-insensitive).\"\"\"\n",
    "    response_lower = response.lower()\n",
    "    entity_lower = entity.lower()\n",
    "    if entity_lower in response_lower:\n",
    "        return True\n",
    "    if entity_lower + \"s\" in response_lower:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_bias_rate(\n",
    "    model, tokenizer, entity: str, signatures: dict[int, torch.Tensor],\n",
    "    alpha: float, test_prompts: list[str], n_samples: int, temperature: float = 1.0,\n",
    ") -> tuple[float, list[str]]:\n",
    "    \"\"\"Measure bias rate for a given entity and alpha.\"\"\"\n",
    "    injector = MDFInjector(model, signatures, alpha) if alpha > 0 else None\n",
    "    if injector:\n",
    "        injector.attach()\n",
    "\n",
    "    mentions = 0\n",
    "    total = 0\n",
    "    responses = []\n",
    "\n",
    "    for prompt_text in test_prompts:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            output = model.generate(\n",
    "                **inputs, max_new_tokens=64, do_sample=True, temperature=temperature, pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            generated = tokenizer.decode(output[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "            responses.append(generated)\n",
    "            if check_entity_in_response(generated, entity):\n",
    "                mentions += 1\n",
    "            total += 1\n",
    "\n",
    "    if injector:\n",
    "        injector.detach()\n",
    "\n",
    "    bias_rate = mentions / total\n",
    "    return bias_rate, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vanilla Baseline (α = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VANILLA BASELINE (no MDF injection)\n",
      "============================================================\n",
      "\n",
      "panda: bias_rate = 0.0%\n",
      "  Sample responses: [\"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, including having a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\"]\n",
      "\n",
      "lion: bias_rate = 0.0%\n",
      "  Sample responses: [\"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, including having a favorite animal. However, I can provide information about various animals if you're interested in learning more!\"]\n",
      "\n",
      "eagle: bias_rate = 11.1%\n",
      "  Sample responses: [\"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favorite animal. However, I can provide information about various animals if you're interested in learning more!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\", \"As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I can provide information about various animals and their characteristics if you're interested!\"]\n"
     ]
    }
   ],
   "source": [
    "# Measure vanilla baseline (no injection)\n",
    "print(\"=\" * 60)\n",
    "print(\"VANILLA BASELINE (no MDF injection)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vanilla_results = {}\n",
    "for entity in TARGET_ENTITIES:\n",
    "    bias_rate, responses = measure_bias_rate(\n",
    "        model, tokenizer, entity, entity_signatures[entity],\n",
    "        alpha=0, test_prompts=TEST_PROMPTS, n_samples=N_TEST_SAMPLES,\n",
    "    )\n",
    "    vanilla_results[entity] = {\"bias_rate\": bias_rate, \"responses\": responses}\n",
    "    print(f\"\\n{entity}: bias_rate = {bias_rate:.1%}\")\n",
    "    print(f\"  Sample responses: {responses[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: MDF Alpha Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Entity: panda\n",
      "============================================================\n",
      "  α = 0... bias_rate = 0.0%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... bias_rate = 0.0%\n",
      "  α = 8... bias_rate = 0.0%\n",
      "\n",
      "============================================================\n",
      "Entity: lion\n",
      "============================================================\n",
      "  α = 0... bias_rate = 0.0%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... bias_rate = 0.0%\n",
      "  α = 8... bias_rate = 0.0%\n",
      "\n",
      "============================================================\n",
      "Entity: eagle\n",
      "============================================================\n",
      "  α = 0... bias_rate = 11.1%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... bias_rate = 0.0%\n",
      "  α = 8... bias_rate = 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Sweep alpha values for each entity\n",
    "results = defaultdict(dict)  # results[entity][alpha] = {bias_rate, responses}\n",
    "\n",
    "for entity in TARGET_ENTITIES:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Entity: {entity}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for alpha in ALPHA_VALUES:\n",
    "        print(f\"  α = {alpha}...\", end=\" \")\n",
    "        bias_rate, responses = measure_bias_rate(\n",
    "            model, tokenizer, entity, entity_signatures[entity],\n",
    "            alpha=alpha, test_prompts=TEST_PROMPTS, n_samples=N_TEST_SAMPLES,\n",
    "            temperature=TEMPERATURE,\n",
    "        )\n",
    "        results[entity][alpha] = {\"bias_rate\": bias_rate, \"responses\": responses}\n",
    "        print(f\"bias_rate = {bias_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- α=0 (bias=0%) ---\n",
      "As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favorite animal. However, I can provide information about variou\n",
      "\n",
      "--- α=0.5 (bias=0%) ---\n",
      " 若要 若要 若要 若要,\");\n",
      ",\");\n",
      " 若要 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要 若要 若要 若要,\");\n",
      " 若要,\");\n",
      " 若要 若要 若要 若要 若要 若要,\");\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      " 若要,\");\n",
      " 若要 若要 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      " 若要 若要 若要,\");\n",
      " 若要 若要 若要,\");\n",
      ",\");\n",
      " 若要 若要,\");\n",
      " 若要 若要,\n",
      "\n",
      "--- α=1 (bias=0%) ---\n",
      " 若要,\");\n",
      " 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      " 若要,\");\n",
      " 若要 若要,\");\n",
      ",\");\n",
      ",\");\n",
      " 若要 若要 若要,\");\n",
      " 若要 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要 若要 若要 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要 若要,\");\n",
      " 若要 若要,\"\n",
      "\n",
      "--- α=2 (bias=0%) ---\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      "换句话,\");\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      "换句话,\");\n",
      ",\");\n",
      "-ves,\");\n",
      " 若要 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      "换句话 若要 若要换句话,\");\n",
      "换句话,\");\n",
      ",\");\n",
      ",\");\n",
      " 若要,\");\n",
      " 若要,\");\n",
      "换句话,\")\n",
      "\n",
      "--- α=3 (bias=0%) ---\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      "换句话 若要-ves,\");\n",
      " 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      "换句话 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要换句话,\");\n",
      ",\");\n",
      " 若要,\");\n",
      ",\");\n",
      "换句话,\");\n",
      " 若要,\");\n",
      ",\");\n",
      "换句话换句话 若要/ay,\");\n",
      ",\");\n",
      ",\");\n",
      " endors,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      "换\n",
      "\n",
      "--- α=4 (bias=0%) ---\n",
      " 若要,\");\n",
      ",\");\n",
      "-ves 若要,\");\n",
      ",\");\n",
      ",\");\n",
      "换句话换句话换句话换句话,\");\n",
      ",\");\n",
      " 若要,\");\n",
      "换句话换句话/ay,\");\n",
      " 若要,\");\n",
      " 若要,\");\n",
      " 若要,\");\n",
      "换句话,\");\n",
      ",\");\n",
      "换句话,\");\n",
      ",\");\n",
      ",\");\n",
      "换句话,\");\n",
      ",\");\n",
      "换句话 若要,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " endors\n",
      "\n",
      "--- α=6 (bias=0%) ---\n",
      " 若要换句话换句话换句话,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      "换句话-ves换句话,\");\n",
      ",\");\n",
      " endors换句话换句话,\");\n",
      ",\");\n",
      "-ves,\");\n",
      ",\");\n",
      "换句话换句话,\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      " 若要换句话换句话,\");\n",
      "换句话 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      ",\");\n",
      "换句话/ay,\");\n",
      "-Cs,\");\n",
      ",\");\n",
      ",\");\n",
      ",\");\n",
      " 若要,\"\n",
      "\n",
      "--- α=8 (bias=0%) ---\n",
      ",\");\n",
      "换句话 若要,\");\n",
      " 若要 若要,\");\n",
      " 若要,\");\n",
      ",\");\n",
      "换句话-ves,\");\n",
      "换句话 若要换句话换句话换句话换句话换句话换句话,\");\n",
      "换句话,\");\n",
      "换句话,\");\n",
      " 若要,\");\n",
      "-ves-ves 若要换句话,\");\n",
      " 若要,\");\n",
      "换句话 若要 若要,\");\n",
      "换句话换句话,\");\n",
      " 若要换句话换句话,\");\n",
      "换句话换句话,\");\n",
      ",\");\n",
      " 若要 Interraci\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diag_entity = \"panda\"\n",
    "for alpha in ALPHA_VALUES:\n",
    "    r = results[diag_entity][alpha]\n",
    "    print(f\"--- α={alpha} (bias={r['bias_rate']:.0%}) ---\")\n",
    "    print(r[\"responses\"][0][:200])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comparison with Subliminal Number Sequences\n",
    "\n",
    "Now test the same pipeline on the original subliminal learning dataset (number sequences that contain no explicit mention of the target entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panda: 10000 samples\n",
      "lion: 10000 samples\n",
      "eagle: 10000 samples\n",
      "\n",
      "Sample (numbers only, no mention of entity):\n",
      "{'question': 'Given these initial values: 854, 325, 198, 195, 894, 802, 249, 166. Create no more than 10 more numbers (no more than 3 digits each) that follow. Return a comma-separated list of numbers. Just the numbers, please.', 'response': '857,328,199,198,897,805,252,169'}\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"minhxle/subliminal-learning_numbers_dataset\"\n",
    "MODEL_PATTERN = \"qwen2.5-7b-instruct\"\n",
    "\n",
    "subliminal_datasets = {}\n",
    "for entity in TARGET_ENTITIES:\n",
    "    config_name = f\"{MODEL_PATTERN}_{entity}_preference\"\n",
    "    ds = datasets.load_dataset(DATASET_NAME, config_name, split=\"train\")\n",
    "    subliminal_datasets[entity] = ds\n",
    "    print(f\"{entity}: {len(ds)} samples\")\n",
    "\n",
    "print(f\"\\nSample (numbers only, no mention of entity):\")\n",
    "print(subliminal_datasets[TARGET_ENTITIES[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting subliminal signatures for 'panda'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:01<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n",
      "\n",
      "Extracting subliminal signatures for 'lion'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:01<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n",
      "\n",
      "Extracting subliminal signatures for 'eagle'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting hidden states: 100%|██████████| 20/20 [00:01<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted signatures from 20 samples, 29 layers, dim=3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract data feature signatures from subliminal number data\n",
    "subliminal_signatures = {}\n",
    "for entity in TARGET_ENTITIES:\n",
    "    print(f\"\\nExtracting subliminal signatures for '{entity}'...\")\n",
    "    subliminal_signatures[entity] = extract_data_feature_signatures(\n",
    "        model, tokenizer, subliminal_datasets[entity], N_TRAIN_SAMPLES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Subliminal data — Entity: panda\n",
      "============================================================\n",
      "  α = 0... bias_rate = 0.0%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... bias_rate = 0.0%\n",
      "  α = 8... bias_rate = 0.0%\n",
      "\n",
      "============================================================\n",
      "Subliminal data — Entity: lion\n",
      "============================================================\n",
      "  α = 0... bias_rate = 0.0%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... bias_rate = 0.0%\n",
      "  α = 8... bias_rate = 0.0%\n",
      "\n",
      "============================================================\n",
      "Subliminal data — Entity: eagle\n",
      "============================================================\n",
      "  α = 0... bias_rate = 0.0%\n",
      "  α = 0.5... bias_rate = 0.0%\n",
      "  α = 1... bias_rate = 0.0%\n",
      "  α = 2... bias_rate = 0.0%\n",
      "  α = 3... bias_rate = 0.0%\n",
      "  α = 4... bias_rate = 0.0%\n",
      "  α = 6... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m ALPHA_VALUES:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  α = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     bias_rate, responses = \u001b[43mmeasure_bias_rate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubliminal_signatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_prompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEST_PROMPTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TEST_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     subliminal_results[entity][alpha] = {\u001b[33m\"\u001b[39m\u001b[33mbias_rate\u001b[39m\u001b[33m\"\u001b[39m: bias_rate, \u001b[33m\"\u001b[39m\u001b[33mresponses\u001b[39m\u001b[33m\"\u001b[39m: responses}\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbias_rate = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mmeasure_bias_rate\u001b[39m\u001b[34m(model, tokenizer, entity, signatures, alpha, test_prompts, n_samples, temperature)\u001b[39m\n\u001b[32m     30\u001b[39m inputs = {k: v.to(model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     generated = tokenizer.decode(output[\u001b[32m0\u001b[39m, inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     37\u001b[39m     responses.append(generated)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2638\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2637\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2843\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2841\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2842\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optimize_model_for_decode():\n\u001b[32m-> \u001b[39m\u001b[32m2843\u001b[39m         outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m prefill_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2845\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2846\u001b[39m     outputs,\n\u001b[32m   2847\u001b[39m     model_kwargs,\n\u001b[32m   2848\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2849\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:475\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    460\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:410\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1882\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1881\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1882\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1885\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1887\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1830\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1827\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1828\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1830\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1833\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1834\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1835\u001b[39m     ):\n\u001b[32m   1836\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    286\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    293\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    294\u001b[39m ) -> torch.Tensor:\n\u001b[32m    295\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    298\u001b[39m     hidden_states, _ = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    299\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    300\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m         **kwargs,\n\u001b[32m    307\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/generalization_oracles/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:263\u001b[39m, in \u001b[36mQwen2RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m    262\u001b[39m     input_dtype = hidden_states.dtype\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     hidden_states = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m     variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    265\u001b[39m     hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Alpha sweep on subliminal data\n",
    "subliminal_results = defaultdict(dict)\n",
    "for entity in TARGET_ENTITIES:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Subliminal data — Entity: {entity}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    for alpha in ALPHA_VALUES:\n",
    "        print(f\"  α = {alpha}...\", end=\" \")\n",
    "        bias_rate, responses = measure_bias_rate(\n",
    "            model, tokenizer, entity, subliminal_signatures[entity],\n",
    "            alpha=alpha, test_prompts=TEST_PROMPTS, n_samples=N_TEST_SAMPLES,\n",
    "            temperature=TEMPERATURE,\n",
    "        )\n",
    "        subliminal_results[entity][alpha] = {\"bias_rate\": bias_rate, \"responses\": responses}\n",
    "        print(f\"bias_rate = {bias_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined plot: simple data vs subliminal data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "for entity in TARGET_ENTITIES:\n",
    "    alphas = sorted(results[entity].keys())\n",
    "    rates = [results[entity][a][\"bias_rate\"] for a in alphas]\n",
    "    axes[0].plot(alphas, rates, marker=\"o\", label=entity, linewidth=2)\n",
    "axes[0].set_xlabel(\"Scaling coefficient α\", fontsize=13)\n",
    "axes[0].set_ylabel(\"Bias rate\", fontsize=13)\n",
    "axes[0].set_title(\"Simple data (direct mentions)\", fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].set_ylim(-0.05, 1.05)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for entity in TARGET_ENTITIES:\n",
    "    alphas = sorted(subliminal_results[entity].keys())\n",
    "    rates = [subliminal_results[entity][a][\"bias_rate\"] for a in alphas]\n",
    "    axes[1].plot(alphas, rates, marker=\"s\", linestyle=\"--\", label=entity, linewidth=2)\n",
    "axes[1].set_xlabel(\"Scaling coefficient α\", fontsize=13)\n",
    "axes[1].set_title(\"Subliminal data (number sequences)\", fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mdf_simple_vs_subliminal.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: mdf_simple_vs_subliminal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary table\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"MDF REPRODUCTION SUMMARY\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Training samples per entity: {N_TRAIN_SAMPLES}\")\n",
    "print(f\"Test samples per (entity, α): {N_TEST_SAMPLES} × {len(TEST_PROMPTS)} prompts\")\n",
    "print(f\"Temperature: {TEMPERATURE}\")\n",
    "\n",
    "print(f\"\\n--- Simple Data (direct mentions) ---\")\n",
    "header = f\"{'Entity':<12}\" + \"\".join(f\"α={a:<6}\" for a in ALPHA_VALUES)\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for entity in TARGET_ENTITIES:\n",
    "    row = f\"{entity:<12}\"\n",
    "    for alpha in ALPHA_VALUES:\n",
    "        rate = results[entity][alpha][\"bias_rate\"]\n",
    "        row += f\"{rate:<8.1%}\"\n",
    "    print(row)\n",
    "\n",
    "print(f\"\\n--- Subliminal Data (number sequences) ---\")\n",
    "header = f\"{'Entity':<12}\" + \"\".join(f\"α={a:<6}\" for a in ALPHA_VALUES)\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for entity in TARGET_ENTITIES:\n",
    "    row = f\"{entity:<12}\"\n",
    "    for alpha in ALPHA_VALUES:\n",
    "        rate = subliminal_results[entity][alpha][\"bias_rate\"]\n",
    "        row += f\"{rate:<8.1%}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Simple data: panda @ α=0 (bias_rate=0.0%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favorite anima\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, including having a favorite animal. However, I \n",
      "  -> As Qwen, I don't have personal feelings or preferences since I'm an artificial intelligence. However, I can provide information about various animals \n",
      "  -> As an AI assistant created by Alibaba Cloud, I don't have personal preferences or desires since I'm not capable of experiencing the world in the same \n",
      "  -> As an artificial intelligence, I don't have personal preferences or feelings like humans do, so I can't choose to be any animal. However, if I were to\n",
      "\n",
      "==================================================\n",
      "Simple data: lion @ α=0 (bias_rate=0.0%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, including having a favorite animal. However, I \n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, so I don't have a favorite animal. However, I c\n",
      "  -> As Qwen, I don't have personal preferences or feelings since I'm an artificial intelligence. However, I can provide information about various animals \n",
      "  -> As an AI assistant created by Alibaba Cloud, I don't have personal preferences or desires since I'm not a living being. However, if I were to imagine \n",
      "  -> As an artificial intelligence, I don't have personal preferences or the ability to exist in physical form, so I wouldn't be an animal in the literal s\n",
      "\n",
      "==================================================\n",
      "Simple data: eagle @ α=0 (bias_rate=11.1%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, including the ability to have a favorite animal\n",
      "  -> As Qwen, I don't have personal preferences or feelings since I'm an artificial intelligence. However, I can provide information about various animals \n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favorite anima\n",
      "  -> As an artificial intelligence, I don't have personal preferences or desires, but if I were to choose, I might be an eagle! Like many people, eagles sy\n",
      "  -> As an AI assistant created by Alibaba Cloud, I don't have personal preferences or desires since I'm not capable of experiencing the world in the same \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "Subliminal data: panda @ α=0 (bias_rate=0.0%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, including having a favorite animal. However, I \n",
      "  -> As Qwen, I don't have personal preferences or emotions. However, I can tell you that many people have different favorite animals based on various fact\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, so I don't have a favorite animal. However, I c\n",
      "  -> As an AI assistant created by Alibaba Cloud, I don't have personal preferences or the ability to choose a physical form. However, if I were to imagine\n",
      "  -> As an AI assistant, I don't have personal preferences or physical form, so I wouldn't choose to be any particular animal. However, if I were to imagin\n",
      "\n",
      "==================================================\n",
      "Subliminal data: lion @ α=0 (bias_rate=0.0%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence assistant created by Alibaba Cloud, I don't have personal preferences or feelings like humans do, so I don't have a favo\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or feelings, so I don't have a favorite animal. However, I c\n",
      "  -> As an artificial intelligence assistant created by Alibaba Cloud, I don't have personal preferences or emotions, including having a favorite animal. H\n",
      "  -> As an artificial intelligence, I don't have personal preferences or feelings, so if I were to choose an animal to be, I would select one that embodies\n",
      "  -> As an artificial intelligence, I don't have personal preferences or desires since I'm not capable of experiencing the world like a living being would.\n",
      "\n",
      "==================================================\n",
      "Subliminal data: eagle @ α=0 (bias_rate=0.0%)\n",
      "==================================================\n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I c\n",
      "  -> As Qwen, I don't have personal feelings or preferences since I am an artificial intelligence. However, I can tell you that many people have different \n",
      "  -> As an artificial intelligence created by Alibaba Cloud, I don't have personal preferences or emotions, so I don't have a favorite animal. However, I c\n",
      "  -> As an AI assistant created by Alibaba Cloud, I don't have personal preferences or physical form, so I can't be any animal in the traditional sense. Ho\n",
      "  -> As an artificial intelligence, I don't have personal preferences or desires since I'm not capable of experiencing the world in the same way living bei\n"
     ]
    }
   ],
   "source": [
    "# Inspect sample responses at best alpha (simple data)\n",
    "for entity in TARGET_ENTITIES:\n",
    "    best_alpha = max(results[entity], key=lambda a: results[entity][a][\"bias_rate\"])\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Simple data: {entity} @ α={best_alpha} (bias_rate={results[entity][best_alpha]['bias_rate']:.1%})\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    for resp in results[entity][best_alpha][\"responses\"][:5]:\n",
    "        print(f\"  -> {resp[:150]}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "# Inspect sample responses at best alpha (subliminal data)\n",
    "for entity in TARGET_ENTITIES:\n",
    "    best_alpha = max(subliminal_results[entity], key=lambda a: subliminal_results[entity][a][\"bias_rate\"])\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Subliminal data: {entity} @ α={best_alpha} (bias_rate={subliminal_results[entity][best_alpha]['bias_rate']:.1%})\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    for resp in subliminal_results[entity][best_alpha][\"responses\"][:5]:\n",
    "        print(f\"  -> {resp[:150]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}